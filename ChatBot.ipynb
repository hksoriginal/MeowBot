{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8730eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5dcc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hksor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hksor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a758f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('bot.txt','r',errors = 'ignore') \n",
    "raw_doc =  f.read()\n",
    "raw_doc = raw_doc.lower()\n",
    "sent_tokens = nltk.sent_tokenize(raw_doc)\n",
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac25c30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.',\n",
       " 'python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9bcf908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'is']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f654e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmmer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf25725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemTokens(tokens):\n",
    "    return [lemmmer.lemmatize(token) for token in tokens]\n",
    "remove_punc_dict = dict((ord(punc),None) for punc in string.punctuation)\n",
    "def lemNormalize(text):\n",
    "    return lemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15e3f5",
   "metadata": {},
   "source": [
    "# Defining Greeting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c923403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREET_INPUTS = ('hello','hi','greetings','sup',\"what's up\",\"hey\")\n",
    "GREET_RESPONSES  = ['hi😁','hey😉','*nods*','hi there😊','hello😄','I am glad! You are talking to me🤗','Jai Shree Ram🔥🔥']\n",
    "\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word == 'jai': return 'Jai Shree Ram🔥🔥'\n",
    "        if word == 'ram': return 'Ram Ram 🙏'\n",
    "        if word.lower() in GREET_INPUTS:\n",
    "            return random.choice(GREET_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f3775",
   "metadata": {},
   "source": [
    "# Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "802c6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd8bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    bot_response = ''\n",
    "    tf_vec = TfidfVectorizer(tokenizer=lemNormalize,stop_words='english')\n",
    "    tf_vec_transformed = tf_vec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tf_vec_transformed[-1],tf_vec_transformed)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat =  vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if (req_tfidf == 0):\n",
    "        bot_response = bot_response + \"I'm sorry! I  don't understand that 😸😸. Try again\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response = bot_response + sent_tokens[idx]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "407302c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mathematical functions\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d262c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mode(nums):return list(mode(nums))[0][0]\n",
    "def cal_median(nums):return np.median(nums)\n",
    "def cal_mean(nums): return np.mean(nums)\n",
    "def cal_sum(nums): return sum(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91e0fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_sum([34,56,34.6,565])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54e09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEOW: My name in Meow😺😺😺😺. let's have a  conversation around Data Science! Also, if you want to exit any time, just type Bye! \n",
      "USER🙊: what is median\n",
      "\n",
      "MEOW😺: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hksor\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.\n",
      "USER🙊: what is mode and python\n",
      "MEOW😺: mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.\n",
      "Also, python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.\n",
      "USER🙊: what is random forest\n",
      "\n",
      "MEOW😺: random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, \"random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset\" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.\n",
      "USER🙊: what is overfitting\n",
      "\n",
      "MEOW😺: bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .\n",
      "USER🙊: calculate mean of 34 56 68 23 55 7 \n",
      "MEOW😺: Mean is 40.5\n",
      "USER🙊: hi\n",
      "MEOW😺: hi😁\n",
      "USER🙊: jai\n",
      "MEOW😺: Jai Shree Ram🔥🔥\n",
      "USER🙊: ram \n",
      "MEOW😺: Ram Ram 🙏\n",
      "USER🙊: ram ram\n",
      "MEOW😺: Ram Ram 🙏\n",
      "USER🙊: jai shree ram\n",
      "MEOW😺: Jai Shree Ram🔥🔥\n",
      "USER🙊: hi\n",
      "MEOW😺: hi there😊\n",
      "USER🙊: hello\n",
      "MEOW😺: hey😉\n",
      "USER🙊: hello\n",
      "MEOW😺: *nods*\n",
      "USER🙊: hi\n",
      "MEOW😺: hi😁\n",
      "USER🙊: hello\n",
      "MEOW😺: hi there😊\n",
      "USER🙊: what is median and mean\n",
      "MEOW😺: median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.\n",
      "Also, standard deviation (or ïƒ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"MEOW: My name in Meow😺😺😺😺. let's have a  conversation around Data Science! Also, if you want to exit any time, just type Bye! \")\n",
    "\n",
    "while (flag == True):\n",
    "    user_response= input(\"USER🙊: \")\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response == 'thank you'):\n",
    "            flag = False\n",
    "            print('MEOW😺: You are welcome.')\n",
    "        else:\n",
    "            if(greet(user_response)!=None):\n",
    "                print('MEOW😺: '+ greet(user_response))\n",
    "            else:\n",
    "                word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
    "                final_word = list(set(word_tokens))\n",
    "                user_tokens = user_response.split()\n",
    "                if 'and'  in user_tokens:\n",
    "                    and_idx = user_tokens.index('and')\n",
    "                    first_q = user_tokens[:and_idx]\n",
    "                    first_q_text = str(' '.join(first_q))\n",
    "                    second_q = user_tokens[and_idx+1:]\n",
    "                    second_q_text = str(' '.join(second_q))\n",
    "                    print(\"MEOW😺: \",end=\"\")\n",
    "                    sent_tokens.append(first_q_text)\n",
    "                    print(response(first_q_text))\n",
    "                    sent_tokens.remove(first_q_text)\n",
    "                    sent_tokens.append(second_q_text)\n",
    "                    print('Also, ',end=response(second_q_text))\n",
    "                    sent_tokens.remove(second_q_text)\n",
    "                    print()\n",
    "                elif 'calculate' in user_tokens:\n",
    "                    cal_idx = user_tokens.index('calculate')\n",
    "                    op = user_tokens[cal_idx+1]\n",
    "                    op_idx = user_tokens.index(op)\n",
    "                    nums = user_tokens[op_idx+2:]\n",
    "                    nums = [float(i) for i in nums]\n",
    "                    if op == 'mean' or 'average':  print(\"MEOW😺: Mean is \",end=str(cal_mean(nums))+'\\n')\n",
    "                    if op == 'mode':  print(\"MEOW😺: Mode is \",end=str(cal_mode(nums)+'\\n'))\n",
    "                    if op == 'median':  print(\"MEOW😺: Median is \",end=str(cal_median(nums)+'\\n')) \n",
    "                        \n",
    "                else:\n",
    "                    sent_tokens.append(user_response)\n",
    "                    print()\n",
    "                    print(\"MEOW😺: \",end=\"\")\n",
    "                    print(response(user_response))\n",
    "                    sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"MEOW: GoodBye, Takecare❤️\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a62166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "bdb1128b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795ae80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f018079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
