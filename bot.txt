python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

a confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known.

accuracy is the number of correctly predicted data points out of all the data points more formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives.

type i and type ii errors are very common in machine learning and statistics type i error occurs when the null hypothesis (h0) is mistakenly rejected this is also referred to as the false positive error type ii error occurs when a null hypothesis that is actually false is accepted.

precision is the number of true positives divided by the number of true positives plus the number of false positives false positives are cases the model incorrectly labels as positive that are actually negative, or in our example, individuals the model classifies as terrorists that are not .

recall expresses the ability to find all relevant instances of a class in a data set, precision expresses the proportion of the data points our model says existed in the relevant class that were indeed relevant.

a false positive is when a scientist determines something is true when it is actually false (also called a type i error) a false positive is a “false alarm”.

a false negative means something that is there was not detected; something was missed.

true positive is an outcome where the model correctly predicts the positive class similarly, a true negative is an outcome where the model correctly predicts the negative class a false positive is an outcome where the model incorrectly predicts the positive class.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

a confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known.

accuracy is the number of correctly predicted data points out of all the data points more formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives.

type i and type ii errors are very common in machine learning and statistics type i error occurs when the null hypothesis (h0) is mistakenly rejected this is also referred to as the false positive error type ii error occurs when a null hypothesis that is actually false is accepted.

precision is the number of true positives divided by the number of true positives plus the number of false positives false positives are cases the model incorrectly labels as positive that are actually negative, or in our example, individuals the model classifies as terrorists that are not .

recall expresses the ability to find all relevant instances of a class in a data set, precision expresses the proportion of the data points our model says existed in the relevant class that were indeed relevant.

a false positive is when a scientist determines something is true when it is actually false (also called a type i error) a false positive is a “false alarm”.

a false negative means something that is there was not detected; something was missed.

true positive is an outcome where the model correctly predicts the positive class similarly, a true negative is an outcome where the model correctly predicts the negative class a false positive is an outcome where the model incorrectly predicts the positive class.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

a confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known.

accuracy is the number of correctly predicted data points out of all the data points more formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives.

type i and type ii errors are very common in machine learning and statistics type i error occurs when the null hypothesis (h0) is mistakenly rejected this is also referred to as the false positive error type ii error occurs when a null hypothesis that is actually false is accepted.

precision is the number of true positives divided by the number of true positives plus the number of false positives false positives are cases the model incorrectly labels as positive that are actually negative, or in our example, individuals the model classifies as terrorists that are not .

recall expresses the ability to find all relevant instances of a class in a data set, precision expresses the proportion of the data points our model says existed in the relevant class that were indeed relevant.

a false positive is when a scientist determines something is true when it is actually false (also called a type i error) a false positive is a “false alarm”.

a false negative means something that is there was not detected; something was missed.

true positive is an outcome where the model correctly predicts the positive class similarly, a true negative is an outcome where the model correctly predicts the negative class a false positive is an outcome where the model incorrectly predicts the positive class.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

a confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known.

accuracy is the number of correctly predicted data points out of all the data points more formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives.

type i and type ii errors are very common in machine learning and statistics type i error occurs when the null hypothesis (h0) is mistakenly rejected this is also referred to as the false positive error type ii error occurs when a null hypothesis that is actually false is accepted.

precision is the number of true positives divided by the number of true positives plus the number of false positives false positives are cases the model incorrectly labels as positive that are actually negative, or in our example, individuals the model classifies as terrorists that are not .

recall expresses the ability to find all relevant instances of a class in a data set, precision expresses the proportion of the data points our model says existed in the relevant class that were indeed relevant.

a false positive is when a scientist determines something is true when it is actually false (also called a type i error) a false positive is a “false alarm”.

a false negative means something that is there was not detected; something was missed.

true positive is an outcome where the model correctly predicts the positive class similarly, a true negative is an outcome where the model correctly predicts the negative class a false positive is an outcome where the model incorrectly predicts the positive class.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

python is a high-level, interpreted, general-purpose programming language being a general-purpose language, it can be used to build almost any type of application with the right tools/libraries additionally, python supports objects, modules, threads, exception-handling, and automatic memory management which help in modelling real-world problems and building applications to solve these problems.

python is an interpreted language, executes each statement line by line and thus type-checking is done on the fly, during execution hence, python is a dynamically typed language.

statistics is the discipline that studies and develops techniques for gathering, processing, analyzing, interpreting, and communicating statistical information.

hypothesis testing is the statistical significance of an experiment's insights can be assessed using hypothesis testing hypothesis testing examines the probability of a given experiment's results occurring by chance the null hypothesis is defined first, and then p-values are computed if the null hypothesis is true, other values are determined as well as its name suggests, the alpha value indicates the degree of significance.

pareto principle states that 80% of the effects or results in an experiment come from 20% of the causes the pareto principle is often applied to business to explain that 80% of the profits or results come from 20% of the efforts to illustrate, 80% of customers buy 20% of the items.

mean is an essential concept in mathematics and statistics the mean is the average or the most common value in a collection of numbers  in statistics, it is a measure of central tendency of a probability distribution along median and mode it is also referred to as an expected value  it is a statistical concept that carries a major significance in finance the concept is used in various financial fields, including but not limited to portfolio management and business valuation.

mode is the value that appears most frequently in a data set a set of data may have one mode, more than one mode, or no mode at all other popular measures of central tendency include the mean, or the average of a set, and the median, the middle value in a set.

p-value It is a number that helps determine the probability of a random occurrence when evaluating a hypothesis In statistics, the p-value indicates how likely it is that a particular dataset occurred by chance. If the p-value is less than alpha, we can conclude that there is a probability of 5% that the experiment results occurred by chance or 5% of the time, we would see these results

cherry-picking is the act of exclusively taking the bits of information that support a particular conclusion and ignoring all the bits of information that contradict it.

p-hacking, also known as data collection or analysis manipulation, is a technique that produces significant patterns even though they have no underlying effect.

data science which is a scientific discipline that employs data, includes interdisciplinary methods, algorithms, and even the procedure for extracting data knowledge the data can be either coded or uncoerced.

median is the middle number in a sorted, ascending or descending list of numbers and can be more descriptive of that data set than the average it is the point above and below which half (50%) the observed data falls, and so represents the midpoint of the data.

sampling is selecting an unbiased or random subset of individual observations in a population is regarded as part of the statistical practice of sampling in order to obtain some understanding of the population, sampling is used.

machine learning is the field of study that gives computers the capability to learn without being explicitly programmed ml is one of the most exciting technologies that one would have ever come across as it is evident from the name, it gives the computer that makes it more similar to humans: the ability to learn machine learning is actively being used today, perhaps in many more places than one would expect.

supervised learning is a type of machine learning method in which we provide sample labeled data to the machine learning system in order to train it, and on that basis, it predicts the output  the system creates a model using labeled data to understand the datasets and learn about each data, once the training and processing are done then we test the model by providing a sample data to check whether it is predicting the exact output or not  the goal of supervised learning is to map input data with the output data the supervised learning is based on supervision, and it is the same as when a student learns things in the supervision of the teacher the example of supervised learning is spam filtering.

unsupervised learning is a learning method in which a machine learns without any supervision  the training is provided to the machine with the set of data that has not been labeled, classified, or categorized, and the algorithm needs to act on that data without any supervision the goal of unsupervised learning is to restructure the input data into new features or a group of objects with similar patterns  in unsupervised learning, we don't have a predetermined result the machine tries to find useful insights from the huge amount of data .

reinforcement learning is a feedback-based learning method, in which a learning agent gets a reward for each right action and gets a penalty for each wrong action the agent learns automatically with these feedbacks and improves its performance in reinforcement learning, the agent interacts with the environment and explores it the goal of an agent is to get the most reward points, and hence, it improves its performance  the robotic dog, which automatically learns the movement of his arms, is an example of reinforcement learning.

linear regression is one of the easiest and most popular machine learning algorithms it is a statistical method that is used for predictive analysis linear regression makes predictions for continuous/real or numeric variables such as sales, salary, age, product price, etc  linear regression algorithm shows a linear relationship between a dependent (y) and one or more independent (y) variables, hence called as linear regression since linear regression shows the linear relationship, which means it finds how the value of the dependent variable is changing according to the value of the independent variable.

logistic regression is one of the most popular machine learning algorithms, which comes under the supervised learning technique it is used for predicting the categorical dependent variable using a given set of independent variables logistic regression predicts the output of a categorical dependent variable therefore the outcome must be a categorical or discrete value it can be either yes or no, 0 or 1, true or false, etc but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.

support vector machine or svm is one of the most popular supervised learning algorithms, which is used for classification as well as regression problems however, primarily, it is used for classification problems in machine learning  the goal of the svm algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future this best decision boundary is called a hyperplane  svm chooses the extreme points/vectors that help in creating the hyperplane these extreme cases are called as support vectors, and hence algorithm is termed as support vector machine.

decision tree is a supervised learning technique that can be used for both classification and regression problems, but mostly it is preferred for solving classification problems it is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome in a decision tree, there are two nodes, which are the decision node and leaf node decision nodes are used to make any decision and have multiple branches, whereas leaf nodes are the output of those decisions and do not contain any further branches the decisions or the test are performed on the basis of features of the given dataset it is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions it is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.

random forest is a popular machine learning algorithm that belongs to the supervised learning technique it can be used for both classification and regression problems in ml it is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model  as the name suggests, "random forest is a classifier that contains a number of decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset" instead of relying on one decision tree, the random forest takes the prediction from each tree and based on the majority votes of predictions, and it predicts the final output  the greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting.

naive bayes algorithm is a supervised learning algorithm, which is based on bayes theorem and used for solving classification problems it is mainly used in text classification that includes a high-dimensional training dataset naïve bayes classifier is one of the simple and most effective classification algorithms which helps in building the fast machine learning models that can make quick predictions it is a probabilistic classifier, which means it predicts on the basis of the probability of an object.

ensemble learning helps improve machine learning results by combining several models this approach allows the production of better predictive performance compared to a single model basic idea is to learn a set of classifiers (experts) and to allow them to vote.

bootstrap aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression it decreases the variance and helps to avoid overfitting it is usually applied to decision tree methods bagging is a special case of the model averaging approach .

boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers it is done by building a model by using weak models in series firstly, a model is built from the training data then the second model is built which tries to correct the errors present in the first model this procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.

bias, the term bias was first introduced by tom mitchell in 1980 in his paper titled, “the need for biases in learning generalizations” the idea of having bias was about model giving importance to some of the features in order to generalize better for the larger dataset with various other attributes bias in ml does help us generalize better and make our model less sensitive to some single data point.

variance is the variability of model prediction for a given data point or a value which tells us spread of our data model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before as a result, such models perform very well on training data but has high error rates on test data.

standard deviation (or σ) is a measure of how dispersed the data is in relation to the mean low standard deviation means data are clustered around the mean, and high standard deviation indicates data are more spread out.

variance refers to a statistical measurement of the spread between numbers in a data set.

deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network in the mid-1960s, alexey grigorevich ivakhnenko published the first general, while working on deep learning network deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.

neural network is a set of algorithms that aims to recognise the relationships in a set of data by mimicking the way that the human brain works the basic idea is to simulate the densely interconnected brain cells in a computer system, such that a program can learn things, recognise patterns and take decisions as a humanlike way.

input layer: this layer accepts input features it provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer .

hidden layer: nodes of this layer are not exposed to the outer world, they are part of the abstraction provided by any neural network the hidden layer performs all sorts of computation on the features entered through the input layer and transfers the result to the output layer .

output layer: this layer bring up the information learned by the network to the outer world .

activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it the purpose of the activation function is to introduce non-linearity into the output of a neuron .

gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks  training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (ai) and computer science applications .

stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.

mini batch gradient descent is considered to be the cross-over between gd and sgd in this approach instead of iterating through the entire dataset or one observation, we split the dataset into small subsets (batches) and compute the gradients for each batch is a hyperparameter that denotes the size of a single batch.

perceptron is a neural network unit that does certain computations to detect features or business intelligence in the input data it is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x).

convolutional neural network, also known as cnn or convnet, is a class of neural networks that specializes in processing data that has a grid-like topology, such as an image a digital image is a binary representation of visual data it contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.

recurrent neural network(rnn) are a type of neural network where the output from previous step are fed as input to the current step.

opencv is a massive open-source library for various fields like computer vision, machine learning, image processing and plays a critical function in real-time operations, which are fundamental in today's systems.

pixel  or picture element is the smallest addressable element in a raster image, or the smallest addressable element in an all points addressable display device; so it is the smallest controllable element of a picture represented on the screen.

a filter provides a measure for how close a patch or a region of the input resembles a feature a feature may be any prominent aspect – a vertical edge, a horizontal edge, an arch, a diagonal, etc a filter acts as a single template or pattern, which, when convolved across the input, finds similarities between the stored template & different locations/regions in the input image.

padding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above this prevents shrinking.

stride is a component of convolutional neural networks, or neural networks tuned for the compression of images and video data stride is a parameter of the neural network's filter that modifies the amount of movement over the image or video.

lemmatization is a text normalization technique used in natural language processing (nlp), that switches any kind of a word to its base root mode lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning.

stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma stemming is important in natural language understanding (nlu) and natural language processing (nlp).

tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning.

a corpus is a collection of authentic text or audio organized into datasets authentic here means text written or audio spoken by a native of the language or dialect a corpus can be made up of everything from newspapers, novels, recipes, radio broadcasts to television shows, movies, and tweets.

a confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known.

accuracy is the number of correctly predicted data points out of all the data points more formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives.

type i and type ii errors are very common in machine learning and statistics type i error occurs when the null hypothesis (h0) is mistakenly rejected this is also referred to as the false positive error type ii error occurs when a null hypothesis that is actually false is accepted.

precision is the number of true positives divided by the number of true positives plus the number of false positives false positives are cases the model incorrectly labels as positive that are actually negative, or in our example, individuals the model classifies as terrorists that are not .

recall expresses the ability to find all relevant instances of a class in a data set, precision expresses the proportion of the data points our model says existed in the relevant class that were indeed relevant.

a false positive is when a scientist determines something is true when it is actually false (also called a type i error) a false positive is a “false alarm”.

a false negative means something that is there was not detected; something was missed.

true positive is an outcome where the model correctly predicts the positive class similarly, a true negative is an outcome where the model correctly predicts the negative class a false positive is an outcome where the model incorrectly predicts the positive class.

